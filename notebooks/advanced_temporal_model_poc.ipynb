{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Temporal Model Proof of Concept\n",
    "\n",
    "This notebook demonstrates a proof-of-concept implementation of a temporal model (LSTM) for predicting hospital readmissions using time-series EHR data from the MIMIC dataset.\n",
    "\n",
    "Unlike traditional ML models that treat features as static, this approach explicitly models the temporal dynamics of patient data, potentially capturing important patterns that develop over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve\n",
    "\n",
    "# Add project root to path for imports\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "# Import project utilities\n",
    "from src.utils import get_logger, load_config, get_data_path\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Time-Series Data\n",
    "\n",
    "For this POC, we'll restructure our data to preserve the temporal nature of patient measurements. Instead of aggregating features over the entire stay, we'll organize them into sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config = load_config()\n",
    "logger = get_logger('temporal_model_poc')\n",
    "\n",
    "# Load processed data\n",
    "data_path = get_data_path(\"processed\", \"combined_features\", config)\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Display basic info\n",
    "print(f\"Loaded data with {data.shape[0]} rows and {data.shape[1]} columns\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this POC, we'll simulate time-series data if the actual data is already aggregated\n",
    "# In a real implementation, you would extract the actual temporal measurements from the MIMIC database\n",
    "\n",
    "def create_temporal_dataset(data, vital_features, lab_features, seq_length=24):\n",
    "    \"\"\"\n",
    "    Create a temporal dataset from the processed data.\n",
    "    \n",
    "    For this POC, we'll simulate temporal data by:\n",
    "    1. Using the existing features as the final values\n",
    "    2. Generating synthetic time series leading up to these values\n",
    "    \n",
    "    Args:\n",
    "        data: DataFrame with processed features\n",
    "        vital_features: List of vital sign feature names\n",
    "        lab_features: List of lab value feature names\n",
    "        seq_length: Number of time steps in each sequence\n",
    "        \n",
    "    Returns:\n",
    "        X_temporal: Dictionary mapping hadm_id to sequence data\n",
    "        y: Series with readmission labels\n",
    "    \"\"\"\n",
    "    # Extract target and patient IDs\n",
    "    y = data['readmission_30day'].copy()\n",
    "    hadm_ids = data['hadm_id'].values\n",
    "    \n",
    "    # Combine vital and lab features\n",
    "    temporal_features = [f for f in data.columns if any(vf in f for vf in vital_features) or \n",
    "                                                 any(lf in f for lf in lab_features)]\n",
    "    \n",
    "    # Create dictionary to store sequences for each admission\n",
    "    X_temporal = {}\n",
    "    \n",
    "    # For each admission, create a synthetic time series\n",
    "    for i, hadm_id in enumerate(hadm_ids):\n",
    "        # Get final values for this admission\n",
    "        final_values = data.loc[i, temporal_features].values.astype(float)\n",
    "        \n",
    "        # Create a sequence leading up to these values\n",
    "        # For simplicity, we'll use a random walk with the final value as the endpoint\n",
    "        sequence = np.zeros((seq_length, len(temporal_features)))\n",
    "        \n",
    "        for j, final_val in enumerate(final_values):\n",
    "            # Start with a value in the healthy range\n",
    "            start_val = final_val * 0.8 + np.random.normal(0, 0.1)\n",
    "            \n",
    "            # Generate a trajectory from start to final value\n",
    "            trajectory = np.linspace(start_val, final_val, seq_length)\n",
    "            \n",
    "            # Add some noise to make it realistic\n",
    "            noise = np.random.normal(0, abs(final_val) * 0.05, seq_length)\n",
    "            trajectory += noise\n",
    "            \n",
    "            # Store in sequence\n",
    "            sequence[:, j] = trajectory\n",
    "        \n",
    "        # Store sequence for this admission\n",
    "        X_temporal[hadm_id] = sequence\n",
    "    \n",
    "    return X_temporal, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define vital sign and lab value features\n",
    "vital_features = ['heart_rate', 'sbp', 'dbp', 'mbp', 'resp_rate', 'temperature', 'spo2']\n",
    "lab_features = ['wbc', 'hgb', 'platelet', 'sodium', 'potassium', 'bicarbonate', 'bun', 'creatinine', 'glucose']\n",
    "\n",
    "# Create temporal dataset\n",
    "X_temporal, y = create_temporal_dataset(data, vital_features, lab_features)\n",
    "\n",
    "# Display an example sequence\n",
    "example_id = list(X_temporal.keys())[0]\n",
    "example_sequence = X_temporal[example_id]\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "for i, feature in enumerate(vital_features[:5]):  # Plot first 5 vital signs\n",
    "    plt.plot(example_sequence[:, i], label=feature)\n",
    "plt.title(f\"Example Temporal Sequence for Admission {example_id}\")\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"Normalized Value\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implement LSTM Model for Temporal Data\n",
    "\n",
    "Now we'll implement an LSTM model that can process these temporal sequences and predict readmission risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define a PyTorch dataset for our temporal data\n",
    "class TemporalEHRDataset(Dataset):\n",
    "    def __init__(self, sequences, labels, hadm_ids):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "        self.hadm_ids = hadm_ids\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        hadm_id = self.hadm_ids[idx]\n",
    "        sequence = self.sequences[hadm_id]\n",
    "        label = self.labels[idx]\n",
    "        return torch.FloatTensor(sequence), torch.FloatTensor([label])\n",
    "\n",
    "# Define the LSTM model\n",
    "class TemporalPatientLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim, \n",
    "            hidden_dim, \n",
    "            num_layers=num_layers, \n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.attention = nn.Linear(hidden_dim, 1)  # Simple attention mechanism\n",
    "        self.classifier = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Process sequence with LSTM\n",
    "        lstm_out, _ = self.lstm(x)  # lstm_out shape: [batch_size, seq_len, hidden_dim]\n",
    "        \n",
    "        # Apply attention to focus on important time steps\n",
    "        attention_weights = torch.softmax(self.attention(lstm_out), dim=1)\n",
    "        context = torch.sum(attention_weights * lstm_out, dim=1)  # Weighted sum\n",
    "        \n",
    "        # Classify\n",
    "        return torch.sigmoid(self.classifier(context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for PyTorch\n",
    "hadm_ids = list(X_temporal.keys())\n",
    "labels = y.values\n",
    "\n",
    "# Split data\n",
    "train_ids, test_ids, train_labels, test_labels = train_test_split(\n",
    "    hadm_ids, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TemporalEHRDataset(X_temporal, train_labels, train_ids)\n",
    "test_dataset = TemporalEHRDataset(X_temporal, test_labels, test_ids)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Initialize model\n",
    "input_dim = X_temporal[hadm_ids[0]].shape[1]  # Number of features\n",
    "hidden_dim = 64\n",
    "model = TemporalPatientLSTM(input_dim, hidden_dim)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Lists to store metrics\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "test_aucs = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for sequences, labels in train_loader:\n",
    "        sequences, labels = sequences.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(sequences)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * sequences.size(0)\n",
    "    \n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sequences, labels in test_loader:\n",
    "            sequences, labels = sequences.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(sequences)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            test_loss += loss.item() * sequences.size(0)\n",
    "            \n",
    "            # Store predictions and labels\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_losses.append(test_loss)\n",
    "    \n",
    "    # Calculate AUC\n",
    "    all_labels = np.array(all_labels).flatten()\n",
    "    all_preds = np.array(all_preds).flatten()\n",
    "    test_auc = roc_auc_score(all_labels, all_preds)\n",
    "    test_aucs.append(test_auc)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "          f\"Train Loss: {train_loss:.4f}, \"\n",
    "          f\"Test Loss: {test_loss:.4f}, \"\n",
    "          f\"Test AUC: {test_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize Results and Compare with Traditional Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, num_epochs+1), train_losses, 'b-', label='Train Loss')\n",
    "plt.plot(range(1, num_epochs+1), test_losses, 'r-', label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Test Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, num_epochs+1), test_aucs, 'g-')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('AUC')\n",
    "plt.title('Test AUC')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with traditional models\n",
    "from src.models.model import ReadmissionModel\n",
    "\n",
    "# Initialize and train traditional models\n",
    "traditional_models = {\n",
    "    \"Logistic Regression\": ReadmissionModel(),\n",
    "    \"Random Forest\": ReadmissionModel(),\n",
    "    \"XGBoost\": ReadmissionModel(),\n",
    "    \"LightGBM\": ReadmissionModel()\n",
    "}\n",
    "\n",
    "# Train each model with its corresponding algorithm\n",
    "for i, (name, model) in enumerate(traditional_models.items()):\n",
    "    algorithm = name.lower().replace(\" \", \"_\")\n",
    "    if algorithm == \"logistic_regression\":\n",
    "        algorithm = \"logistic_regression\"\n",
    "    metrics = model.fit(data, algorithm=algorithm)\n",
    "    print(f\"{name} AUC: {metrics['roc_auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyze Attention Weights for Interpretability\n",
    "\n",
    "One advantage of our temporal model is the ability to interpret which time points were most important for the prediction through the attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract attention weights\n",
    "def get_attention_weights(model, sequence):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        sequence_tensor = torch.FloatTensor(sequence).unsqueeze(0).to(device)\n",
    "        lstm_out, _ = model.lstm(sequence_tensor)\n",
    "        attention_weights = torch.softmax(model.attention(lstm_out), dim=1)\n",
    "    return attention_weights.cpu().numpy().squeeze()\n",
    "\n",
    "# Get a sample sequence from a readmitted patient\n",
    "readmitted_ids = [id for id, label in zip(test_ids, test_labels) if label == 1]\n",
    "if readmitted_ids:\n",
    "    sample_id = readmitted_ids[0]\n",
    "    sample_sequence = X_temporal[sample_id]\n",
    "    \n",
    "    # Get attention weights\n",
    "    attention_weights = get_attention_weights(model, sample_sequence)\n",
    "    \n",
    "    # Plot sequence with attention weights\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    # Plot vital signs\n",
    "    plt.subplot(2, 1, 1)\n",
    "    for i, feature in enumerate(vital_features[:5]):  # Plot first 5 vital signs\n",
    "        plt.plot(sample_sequence[:, i], label=feature)\n",
    "    plt.title(f\"Vital Signs for Readmitted Patient {sample_id}\")\n",
    "    plt.xlabel(\"Time Step\")\n",
    "    plt.ylabel(\"Normalized Value\")\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot attention weights\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.bar(range(len(attention_weights)), attention_weights)\n",
    "    plt.title(\"Attention Weights\")\n",
    "    plt.xlabel(\"Time Step\")\n",
    "    plt.ylabel(\"Weight\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Identify the most important time steps\n",
    "    top_indices = np.argsort(-attention_weights)[:5]\n",
    "    print(\"Most important time steps:\")\n",
    "    for idx in top_indices:\n",
    "        print(f\"Time step {idx}: Weight {attention_weights[idx]:.4f}\")\n",
    "else:\n",
    "    print(\"No readmitted patients in the test set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Discussion and Conclusions\n",
    "\n",
    "### Advantages of Temporal Modeling\n",
    "\n",
    "1. **Capturing Temporal Patterns**: The LSTM model can identify patterns in how vital signs and lab values change over time, potentially detecting deterioration or improvement trends that static models would miss.\n",
    "\n",
    "2. **Attention Mechanism**: The attention weights provide interpretability by highlighting which time points were most important for the prediction, which could help clinicians understand when critical changes occurred.\n",
    "\n",
    "3. **Handling Variable-Length Sequences**: Although not fully implemented in this POC, LSTM models can naturally handle variable-length sequences, accommodating different hospital stay durations.\n",
    "\n",
    "### Limitations and Future Work\n",
    "\n",
    "1. **Data Requirements**: Temporal models require more detailed data with timestamps, which may not always be available or may require additional preprocessing.\n",
    "\n",
    "2. **Computational Complexity**: Training LSTM models is more computationally intensive than traditional ML models.\n",
    "\n",
    "3. **Hyperparameter Tuning**: These models have more hyperparameters to tune, including LSTM layers, hidden dimensions, and attention mechanisms.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Real Temporal Data**: Replace the synthetic temporal data with actual time-series measurements from the MIMIC database.\n",
    "\n",
    "2. **More Sophisticated Architectures**: Explore bidirectional LSTMs, transformer models, or temporal convolutional networks.\n",
    "\n",
    "3. **Multi-modal Integration**: Combine temporal data with static features (demographics, comorbidities) for a more comprehensive model.\n",
    "\n",
    "4. **Clinical Validation**: Work with clinicians to validate the patterns identified by the attention mechanism."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
