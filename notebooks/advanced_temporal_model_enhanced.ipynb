{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced Temporal Model: Time-Aware LSTM for Readmission Prediction\n",
    "\n",
    "**Objective:** This notebook builds upon the initial LSTM proof-of-concept (`advanced_temporal_model_poc.ipynb`) by implementing and evaluating a **Time-Aware LSTM**. The key enhancement is the explicit modeling of irregular time intervals between clinical measurements, a crucial aspect of real-world EHR data often ignored by standard sequence models.\n",
    "\n",
    "**Narrative:** While traditional ML models (Logistic Regression, LightGBM) provide a baseline, they treat patient data statically. Our first LSTM PoC introduced sequence modeling but didn't fully leverage the *timing* information. This enhanced model incorporates learned time embeddings, aiming to capture the significance of *when* events occur relative to each other. We will train this model and compare its performance (ROC AUC, PR AUC) against a strong baseline (LightGBM with SMOTE, trained on the *same data split*) to assess the potential value added by temporal awareness, even on the limited MIMIC-III Demo dataset.\n",
    "\n",
    "**Methodology:**\n",
    "1. Load processed data (`combined_features.csv`).\n",
    "2. Prepare temporal data: Generate synthetic sequences *with explicit, irregular time intervals*.\n",
    "3. Split data into training and testing sets.\n",
    "4. Define the Time-Aware LSTM architecture using PyTorch (including `TimeEncoder`).\n",
    "5. Implement a PyTorch `Dataset` and `DataLoader` with appropriate padding for sequences and intervals.\n",
    "6. Train the Time-Aware LSTM model.\n",
    "7. Train a baseline LightGBM model on the *same* train/test split for fair comparison (using static/aggregated features).\n",
    "8. Evaluate both models using ROC AUC and Precision-Recall AUC.\n",
    "9. Visualize the results: Training curves, ROC/PR comparison.\n",
    "10. Analyze attention weights in the context of time intervals.\n",
    "11. Discuss findings, limitations, and implications for the MLOps pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve, roc_curve, classification_report\n",
    "import lightgbm as lgb # Baseline model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm # Use notebook version of tqdm\n",
    "\n",
    "# Add project root to path for imports\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Import project utilities and model definitions\n",
    "from src.utils import get_logger, load_config, get_data_path\n",
    "from src.models.temporal_modeling import TimeEncoder, TimeAwarePatientLSTM, TemporalEHRDataset, get_attention_weights # Assuming these are defined here or imported\n",
    "\n",
    "# --- Configuration ---\n",
    "try:\n",
    "    config = load_config()\n",
    "    logger = get_logger('temporal_model_enhanced_nb') # Use specific logger name\n",
    "except FileNotFoundError:\n",
    "    logger.error(\"Configuration file not found. Please ensure 'configs/config.yaml' exists.\")\n",
    "    # Provide default config or raise error if necessary\n",
    "    config = {}\n",
    "\n",
    "# Plotting setup\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6) # Adjusted default size\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Define output directory for results\n",
    "results_dir = os.path.join(os.getcwd(), 'results') # Save results within notebooks dir\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "torch.manual_seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Load processed data (static/aggregated features)\n",
    "    data_path = get_data_path(\"processed\", \"combined_features\", config)\n",
    "    data = pd.read_csv(data_path)\n",
    "    logger.info(f\"Loaded data from {data_path}. Shape: {data.shape}\")\n",
    "    # Display basic info\n",
    "    print(f\"Loaded data with {data.shape[0]} rows and {data.shape[1]} columns\")\n",
    "    display(data.head())\n",
    "except FileNotFoundError:\n",
    "    logger.error(f\"Processed data file not found at {data_path}. Cannot proceed.\")\n",
    "    # Handle error appropriately, e.g., raise Exception or exit\n",
    "    data = pd.DataFrame() # Assign empty df to prevent further errors\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading data: {e}\", exc_info=True)\n",
    "    data = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation for Temporal Model\n",
    "\n",
    "We need to restructure the data into sequences of measurements over time, including the time intervals *between* measurements. As the `combined_features.csv` contains aggregated data, we'll generate synthetic sequences for this demonstration, simulating irregular measurement intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_temporal_dataset_with_intervals(data, vital_features, lab_features, seq_length=24):\n",
    "    \"\"\"\n",
    "    Creates a temporal dataset with explicit time intervals.\n",
    "    Generates synthetic sequences ending at the aggregated values from 'data'.\n",
    "    Simulates irregular intervals between measurements.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): DataFrame with processed aggregated features.\n",
    "        vital_features (List[str]): Base names of vital sign features.\n",
    "        lab_features (List[str]): Base names of lab value features.\n",
    "        seq_length (int): Number of time steps in each sequence.\n",
    "        \n",
    "    Returns:\n",
    "        X_temporal (Dict[str, np.ndarray]): Dictionary mapping hadm_id to sequence data [seq_length, num_features].\n",
    "        time_intervals (Dict[str, np.ndarray]): Dictionary mapping hadm_id to time interval data [seq_length, 1]. Intervals represent hours since previous measurement (first is 0).\n",
    "        timestamps (Dict[str, np.ndarray]): Dictionary mapping hadm_id to cumulative timestamp data [seq_length, 1]. Timestamps represent hours since admission.\n",
    "        y (pd.Series): Series with readmission labels, indexed by hadm_id.\n",
    "        temporal_feature_names (List[str]): List of feature names included in the sequences.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Creating synthetic temporal dataset with sequence length {seq_length}...\")\n",
    "    # Extract target and patient IDs\n",
    "    target_col = 'readmission_30day'\n",
    "    if target_col not in data.columns:\n",
    "        raise ValueError(f\"Target column '{target_col}' not found in data.\")\n",
    "    y = data.set_index('hadm_id')[target_col].copy()\n",
    "    hadm_ids = data['hadm_id'].values\n",
    "\n",
    "    # Identify relevant feature columns based on base names (more robust)\n",
    "    # Assumes aggregated features have names like 'heart_rate_mean', 'glucose_max', etc.\n",
    "    all_feature_cols = [f for f in data.columns if any(vf in f for vf in vital_features) or any(lf in f for lf in lab_features)]\n",
    "    \n",
    "    # Determine a single 'final value' for each base feature (e.g., use mean if available, else max, etc.)\n",
    "    temporal_feature_map = {}\n",
    "    final_value_cols = []\n",
    "    for base_feat in vital_features + lab_features:\n",
    "        found_col = None\n",
    "        for suffix in ['_mean', '_max', '_min', '_last', '']:\n",
    "             potential_col = f\"{base_feat}{suffix}\"\n",
    "             if potential_col in all_feature_cols:\n",
    "                  found_col = potential_col\n",
    "                  break\n",
    "        if found_col:\n",
    "             temporal_feature_map[base_feat] = found_col # Map base name to the column used for final value\n",
    "             final_value_cols.append(found_col)\n",
    "        else:\n",
    "             logger.warning(f\"No suitable aggregated column found for base feature: {base_feat}\")\n",
    "\n",
    "    temporal_feature_names = list(temporal_feature_map.keys()) # Use base names for consistency\n",
    "    num_features = len(temporal_feature_names)\n",
    "    logger.info(f\"Generating sequences for {num_features} features: {temporal_feature_names}\")\n",
    "\n",
    "    X_temporal = {}\n",
    "    time_intervals = {}\n",
    "    timestamps = {}\n",
    "\n",
    "    # Use tqdm for progress tracking\n",
    "    for i in tqdm(range(len(data)), desc=\"Generating Sequences\"):\n",
    "        hadm_id = data.loc[i, 'hadm_id']\n",
    "        \n",
    "        # Generate time intervals (hours since previous measurement)\n",
    "        intervals = np.zeros(seq_length)\n",
    "        # Simulate irregular intervals (e.g., 1-8 hours)\n",
    "        intervals[1:] = np.random.uniform(1, 8, seq_length - 1)\n",
    "        cumulative_timestamps = np.cumsum(intervals)\n",
    "\n",
    "        sequence = np.zeros((seq_length, num_features))\n",
    "\n",
    "        for j, base_feat_name in enumerate(temporal_feature_names):\n",
    "            final_val_col = temporal_feature_map[base_feat_name]\n",
    "            final_val = data.loc[i, final_val_col]\n",
    "\n",
    "            # Handle potential NaN final values (e.g., replace with 0 or median)\n",
    "            if pd.isna(final_val):\n",
    "                final_val = 0 # Simple imputation for demo\n",
    "\n",
    "            # Simulate a plausible starting value (e.g., 80% of final + noise)\n",
    "            start_val = final_val * 0.8 + np.random.normal(0, abs(final_val * 0.1) + 1e-6)\n",
    "            \n",
    "            # Generate a non-linear trajectory from start to final value\n",
    "            progress = np.linspace(0, 1, seq_length) ** 1.5 \n",
    "            trajectory = start_val + (final_val - start_val) * progress\n",
    "            \n",
    "            # Add random noise\n",
    "            noise = np.random.normal(0, abs(final_val * 0.05) + 1e-6, seq_length)\n",
    "            sequence[:, j] = trajectory + noise\n",
    "        \n",
    "        X_temporal[hadm_id] = sequence\n",
    "        time_intervals[hadm_id] = intervals.reshape(-1, 1)\n",
    "        timestamps[hadm_id] = cumulative_timestamps.reshape(-1, 1)\n",
    "    \n",
    "    logger.info(\"Synthetic temporal dataset generation complete.\")\n",
    "    return X_temporal, time_intervals, timestamps, y, temporal_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature sets (base names)\n",
    "# These should ideally come from config, but hardcoding for notebook clarity\n",
    "vital_features = ['heart_rate', 'sbp', 'dbp', 'mbp', 'resp_rate', 'temperature', 'spo2', 'gcs']\n",
    "lab_features = [\n",
    "    'aniongap', 'albumin', 'bands', 'bicarbonate', 'bilirubin', 'bun', \n",
    "    'calcium', 'chloride', 'creatinine', 'glucose', 'hematocrit', 'hemoglobin', \n",
    "    'lactate', 'platelet', 'potassium', 'ptt', 'inr', 'pt', 'sodium', 'wbc'\n",
    "]\n",
    "\n",
    "if not data.empty:\n",
    "    # Create temporal dataset\n",
    "    X_temporal, time_intervals, timestamps, y, temporal_feature_names = create_temporal_dataset_with_intervals(\n",
    "        data, vital_features, lab_features, seq_length=24 # Using 24 time steps for POC\n",
    "    )\n",
    "\n",
    "    # Get admission IDs\n",
    "    hadm_ids = list(X_temporal.keys())\n",
    "    labels = y.loc[hadm_ids].values # Ensure labels align with the keys\n",
    "\n",
    "    # Split data (use same random state for consistency across runs/comparisons)\n",
    "    train_ids, test_ids, train_labels, test_labels = train_test_split(\n",
    "        hadm_ids, labels, test_size=0.2, random_state=RANDOM_STATE, stratify=labels\n",
    "    )\n",
    "    logger.info(f\"Data split: {len(train_ids)} train samples, {len(test_ids)} test samples.\")\n",
    "\n",
    "    # Display an example sequence and its intervals\n",
    "    example_id = train_ids[0]\n",
    "    example_sequence = X_temporal[example_id]\n",
    "    example_intervals = time_intervals[example_id]\n",
    "    example_timestamps = timestamps[example_id]\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    # Plot first few features against cumulative time\n",
    "    num_features_to_plot = min(5, len(temporal_feature_names))\n",
    "    for i in range(num_features_to_plot):\n",
    "        plt.plot(example_timestamps, example_sequence[:, i], label=temporal_feature_names[i], marker='o', linestyle='--')\n",
    "    plt.title(f\"Example Temporal Sequence (Features vs. Time Since Admission) - Admission {example_id}\")\n",
    "    plt.xlabel(\"Time Since Admission (Hours)\")\n",
    "    plt.ylabel(\"Simulated Value (Arbitrary Units)\")\n",
    "    plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "     logger.error(\"Data loading failed earlier, cannot prepare temporal data.\")\n",
    "     # Set dummy variables to avoid errors later, though execution should ideally stop\n",
    "     X_temporal, time_intervals, timestamps, y, temporal_feature_names = {}, {}, {}, pd.Series(), []\n",
    "     train_ids, test_ids, train_labels, test_labels = [], [], [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Definition (Time-Aware LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using TimeEncoder and TimeAwarePatientLSTM from src.models.temporal_modeling\n",
    "# These classes should be defined in that file or copied here for self-containment.\n",
    "# Assuming they are imported correctly.\n",
    "logger.info(\"Using TimeEncoder and TimeAwarePatientLSTM classes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedTemporalEHRDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for temporal sequences with time intervals.\"\"\"\n",
    "    def __init__(self, sequences, time_intervals, labels, hadm_ids):\n",
    "        self.sequences = sequences\n",
    "        self.time_intervals = time_intervals\n",
    "        self.labels = labels\n",
    "        self.hadm_ids = hadm_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        hadm_id = self.hadm_ids[idx]\n",
    "        sequence = self.sequences[hadm_id]\n",
    "        interval = self.time_intervals[hadm_id]\n",
    "        label = self.labels[idx]\n",
    "        return {\n",
    "            'sequence': torch.FloatTensor(sequence),\n",
    "            'intervals': torch.FloatTensor(interval),\n",
    "            'label': torch.FloatTensor([label]) # Label as float tensor\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Pads sequences and intervals in a batch.\"\"\"\n",
    "    # Sort batch by sequence length (optional but common)\n",
    "    # batch.sort(key=lambda x: len(x['sequence']), reverse=True)\n",
    "    \n",
    "    sequences = [item['sequence'] for item in batch]\n",
    "    intervals = [item['intervals'] for item in batch]\n",
    "    labels = [item['label'] for item in batch]\n",
    "\n",
    "    # Pad sequences and intervals\n",
    "    padded_sequences = nn.utils.rnn.pad_sequence(sequences, batch_first=True, padding_value=0.0)\n",
    "    padded_intervals = nn.utils.rnn.pad_sequence(intervals, batch_first=True, padding_value=0.0)\n",
    "    \n",
    "    # Stack labels\n",
    "    labels_tensor = torch.stack(labels)\n",
    "\n",
    "    return {\n",
    "        'sequences': padded_sequences,\n",
    "        'intervals': padded_intervals,\n",
    "        'labels': labels_tensor\n",
    "    }\n",
    "\n",
    "if train_ids: # Only create datasets if data splitting was successful\n",
    "    # Create datasets\n",
    "    train_dataset = EnhancedTemporalEHRDataset(X_temporal, time_intervals, train_labels, train_ids)\n",
    "    test_dataset = EnhancedTemporalEHRDataset(X_temporal, time_intervals, test_labels, test_ids)\n",
    "\n",
    "    # Create dataloaders\n",
    "    batch_size = config.get('models', {}).get('temporal_readmission', {}).get('batch_size', 32) # Get from config or default\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    logger.info(f\"Created DataLoaders with batch size: {batch_size}\")\n",
    "else:\n",
    "     logger.warning(\"Skipping DataLoader creation due to empty train_ids.\")\n",
    "     train_loader, test_loader = None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train the Time-Aware LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_test_results = {\n",
    "    'labels': None, \n",
    "    'preds_proba': None,\n",
    "    'roc_auc': np.nan,\n",
    "    'pr_auc': np.nan\n",
    "}\n",
    "\n",
    "if train_loader and test_loader: # Check if loaders were created\n",
    "    # Initialize model\n",
    "    input_dim = len(temporal_feature_names)\n",
    "    hidden_dim = config.get('models', {}).get('temporal_readmission', {}).get('hidden_dim', 64)\n",
    "    num_layers = config.get('models', {}).get('temporal_readmission', {}).get('num_layers', 1)\n",
    "    time_embed_dim = config.get('models', {}).get('temporal_readmission', {}).get('time_embed_dim', 16)\n",
    "    dropout = config.get('models', {}).get('temporal_readmission', {}).get('dropout', 0.2)\n",
    "    \n",
    "    # Note: TimeAwarePatientLSTM expects num_static_features, but we are not using them in this notebook's dataset.\n",
    "    # We should adapt the model or dataset if static features are needed alongside temporal.\n",
    "    # For now, we'll need a modified model or pass num_static_features=0.\n",
    "    # Let's modify the call assuming the model can handle num_static_features=0 or modify model definition.\n",
    "    # Assuming a simplified LSTM for this notebook focusing only on time-aware sequences:\n",
    "    class SimplifiedTimeAwareLSTM(nn.Module):\n",
    "        def __init__(self, input_dim, hidden_dim, time_embed_dim=16, num_layers=1, dropout=0.2):\n",
    "            super().__init__()\n",
    "            self.time_encoder = TimeEncoder(time_embed_dim)\n",
    "            self.lstm = nn.LSTM(\n",
    "                input_dim + time_embed_dim, hidden_dim, num_layers=num_layers, \n",
    "                batch_first=True, dropout=(dropout if num_layers > 1 else 0)\n",
    "            )\n",
    "            self.attention = nn.Sequential(nn.Linear(hidden_dim, 32), nn.Tanh(), nn.Linear(32, 1))\n",
    "            self.classifier = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        def forward(self, x, time_intervals):\n",
    "            time_encoding = self.time_encoder(time_intervals)\n",
    "            x_with_time = torch.cat([x, time_encoding], dim=2)\n",
    "            lstm_out, _ = self.lstm(x_with_time)\n",
    "            attention_weights = torch.softmax(self.attention(lstm_out), dim=1)\n",
    "            context = torch.sum(attention_weights * lstm_out, dim=1)\n",
    "            # Return logits (BCEWithLogitsLoss expects logits)\n",
    "            return self.classifier(context)\n",
    "\n",
    "    model = SimplifiedTimeAwareLSTM(input_dim, hidden_dim, time_embed_dim, num_layers, dropout)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    logger.info(f\"Initialized SimplifiedTimeAwareLSTM model on {device}.\")\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.BCEWithLogitsLoss() # Use Logits loss\n",
    "    learning_rate = config.get('models', {}).get('temporal_readmission', {}).get('learning_rate', 0.001)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = config.get('models', {}).get('temporal_readmission', {}).get('num_epochs', 15) # Increase epochs slightly\n",
    "    train_losses, test_losses, test_roc_aucs, test_pr_aucs = [], [], [], []\n",
    "\n",
    "    logger.info(f\"Starting LSTM training for {num_epochs} epochs...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        epoch_train_loss = 0.0\n",
    "        train_progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\", leave=False)\n",
    "        for batch in train_progress_bar:\n",
    "            sequences = batch['sequences'].to(device)\n",
    "            intervals = batch['intervals'].to(device)\n",
    "            labels = batch['labels'].to(device).float() # Ensure labels are float\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(sequences, intervals) # Get logits\n",
    "            loss = criterion(outputs.squeeze(), labels.squeeze()) # Squeeze outputs and labels\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_train_loss += loss.item()\n",
    "            train_progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        avg_train_loss = epoch_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Evaluation phase\n",
    "        model.eval()\n",
    "        epoch_test_loss = 0.0\n",
    "        all_labels_list = []\n",
    "        all_preds_proba_list = []\n",
    "        test_progress_bar = tqdm(test_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Eval]\", leave=False)\n",
    "        with torch.no_grad():\n",
    "            for batch in test_progress_bar:\n",
    "                sequences = batch['sequences'].to(device)\n",
    "                intervals = batch['intervals'].to(device)\n",
    "                labels = batch['labels'].to(device).float()\n",
    "\n",
    "                outputs = model(sequences, intervals)\n",
    "                loss = criterion(outputs.squeeze(), labels.squeeze())\n",
    "                epoch_test_loss += loss.item()\n",
    "                \n",
    "                # Apply sigmoid to logits to get probabilities\n",
    "                probabilities = torch.sigmoid(outputs).squeeze()\n",
    "                \n",
    "                all_labels_list.append(labels.cpu().numpy())\n",
    "                # Handle cases where batch size is 1 and probabilities might become scalar\n",
    "                if probabilities.ndim == 0:\n",
    "                    probabilities = probabilities.unsqueeze(0)\n",
    "                all_preds_proba_list.append(probabilities.cpu().numpy())\n",
    "\n",
    "        avg_test_loss = epoch_test_loss / len(test_loader)\n",
    "        test_losses.append(avg_test_loss)\n",
    "        \n",
    "        # Concatenate results from all batches\n",
    "        epoch_labels = np.concatenate([lbl.flatten() for lbl in all_labels_list])\n",
    "        epoch_preds_proba = np.concatenate([prob.flatten() for prob in all_preds_proba_list])\n",
    "        \n",
    "        # Calculate metrics\n",
    "        epoch_roc_auc = roc_auc_score(epoch_labels, epoch_preds_proba)\n",
    "        epoch_pr_auc = average_precision_score(epoch_labels, epoch_preds_proba)\n",
    "        test_roc_aucs.append(epoch_roc_auc)\n",
    "        test_pr_aucs.append(epoch_pr_auc)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}, Test ROC AUC: {epoch_roc_auc:.4f}, Test PR AUC: {epoch_pr_auc:.4f}\")\n",
    "    \n",
    "    logger.info(\"LSTM training complete.\")\n",
    "    \n",
    "    # Store final test results\n",
    "    lstm_test_results['labels'] = epoch_labels\n",
    "    lstm_test_results['preds_proba'] = epoch_preds_proba\n",
    "    lstm_test_results['roc_auc'] = epoch_roc_auc\n",
    "    lstm_test_results['pr_auc'] = epoch_pr_auc\n",
    "    \n",
    "    # Plot training curves\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, num_epochs + 1), train_losses, label='Train Loss')\n",
    "    plt.plot(range(1, num_epochs + 1), test_losses, label='Test Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss (BCEWithLogits)')\n",
    "    plt.title('Training and Test Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, num_epochs + 1), test_roc_aucs, label='Test ROC AUC')\n",
    "    plt.plot(range(1, num_epochs + 1), test_pr_aucs, label='Test PR AUC')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('AUC Score')\n",
    "    plt.title('Test AUC Scores')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(results_dir, 'time_aware_lstm_training_curves.png'))\n",
    "    plt.show()\n",
    "else:\n",
    "    logger.error(\"Cannot train LSTM model as DataLoaders were not created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train Baseline Model (LightGBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_test_results = {\n",
    "    'labels': None, \n",
    "    'preds_proba': None,\n",
    "    'roc_auc': np.nan,\n",
    "    'pr_auc': np.nan\n",
    "}\n",
    "\n",
    "if not data.empty:\n",
    "    # Prepare data for LightGBM (using static/aggregated features)\n",
    "    # Ensure we use the *exact same* train/test split based on hadm_id\n",
    "    target_col = 'readmission_30day'\n",
    "    feature_cols = [col for col in data.columns if col not in ['hadm_id', 'subject_id', 'admittime', 'dischtime', target_col]]\n",
    "    \n",
    "    # Filter data based on train/test IDs\n",
    "    train_data_lgbm = data[data['hadm_id'].isin(train_ids)].copy()\n",
    "    test_data_lgbm = data[data['hadm_id'].isin(test_ids)].copy()\n",
    "    \n",
    "    X_train_lgbm = train_data_lgbm[feature_cols]\n",
    "    y_train_lgbm = train_data_lgbm[target_col]\n",
    "    X_test_lgbm = test_data_lgbm[feature_cols]\n",
    "    y_test_lgbm = test_data_lgbm[target_col]\n",
    "    \n",
    "    # Handle potential NaNs (simple mean imputation for baseline)\n",
    "    X_train_lgbm = X_train_lgbm.fillna(X_train_lgbm.mean())\n",
    "    X_test_lgbm = X_test_lgbm.fillna(X_train_lgbm.mean()) # Use train mean for test set\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_lgbm_scaled = scaler.fit_transform(X_train_lgbm)\n",
    "    X_test_lgbm_scaled = scaler.transform(X_test_lgbm)\n",
    "    \n",
    "    # Define LightGBM parameters (should ideally come from config/tuning)\n",
    "    lgbm_params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'n_estimators': 1000,\n",
    "        'learning_rate': 0.05,\n",
    "        'num_leaves': 31,\n",
    "        'max_depth': -1,\n",
    "        'seed': RANDOM_STATE,\n",
    "        'n_jobs': -1,\n",
    "        'verbose': -1, # Suppress verbose output\n",
    "        'colsample_bytree': 0.8,\n",
    "        'subsample': 0.8,\n",
    "        'reg_alpha': 0.1,\n",
    "        'reg_lambda': 0.1\n",
    "        # Add scale_pos_weight if not using SMOTE, or handle imbalance separately\n",
    "    }\n",
    "    \n",
    "    # Train LightGBM model\n",
    "    logger.info(\"Training LightGBM baseline model...\")\n",
    "    lgbm_model = lgb.LGBMClassifier(**lgbm_params)\n",
    "    lgbm_model.fit(X_train_lgbm_scaled, y_train_lgbm, \n",
    "                   eval_set=[(X_test_lgbm_scaled, y_test_lgbm)], \n",
    "                   eval_metric='auc', \n",
    "                   callbacks=[lgb.early_stopping(100, verbose=False)]) # Early stopping\n",
    "    \n",
    "    # Evaluate LightGBM model\n",
    "    lgbm_preds_proba = lgbm_model.predict_proba(X_test_lgbm_scaled)[:, 1]\n",
    "    lgbm_roc_auc = roc_auc_score(y_test_lgbm, lgbm_preds_proba)\n",
    "    lgbm_pr_auc = average_precision_score(y_test_lgbm, lgbm_preds_proba)\n",
    "    \n",
    "    lgbm_test_results['labels'] = y_test_lgbm.values\n",
    "    lgbm_test_results['preds_proba'] = lgbm_preds_proba\n",
    "    lgbm_test_results['roc_auc'] = lgbm_roc_auc\n",
    "    lgbm_test_results['pr_auc'] = lgbm_pr_auc\n",
    "    \n",
    "    logger.info(f\"LightGBM Baseline - Test ROC AUC: {lgbm_roc_auc:.4f}, Test PR AUC: {lgbm_pr_auc:.4f}\")\n",
    "else:\n",
    "    logger.error(\"Cannot train LightGBM model as data loading failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluate and Compare Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot ROC Curves\n",
    "plt.subplot(1, 2, 1)\n",
    "if lgbm_test_results['labels'] is not None:\n",
    "    fpr_lgbm, tpr_lgbm, _ = roc_curve(lgbm_test_results['labels'], lgbm_test_results['preds_proba'])\n",
    "    plt.plot(fpr_lgbm, tpr_lgbm, label=f\"LightGBM (AUC = {lgbm_test_results['roc_auc']:.3f})\")\n",
    "\n",
    "if lstm_test_results['labels'] is not None:\n",
    "    fpr_lstm, tpr_lstm, _ = roc_curve(lstm_test_results['labels'], lstm_test_results['preds_proba'])\n",
    "    plt.plot(fpr_lstm, tpr_lstm, label=f\"Time-Aware LSTM (AUC = {lstm_test_results['roc_auc']:.3f})\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Chance')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot Precision-Recall Curves\n",
    "plt.subplot(1, 2, 2)\n",
    "if lgbm_test_results['labels'] is not None:\n",
    "    precision_lgbm, recall_lgbm, _ = precision_recall_curve(lgbm_test_results['labels'], lgbm_test_results['preds_proba'])\n",
    "    plt.plot(recall_lgbm, precision_lgbm, label=f\"LightGBM (AUC = {lgbm_test_results['pr_auc']:.3f})\")\n",
    "\n",
    "if lstm_test_results['labels'] is not None:\n",
    "    precision_lstm, recall_lstm, _ = precision_recall_curve(lstm_test_results['labels'], lstm_test_results['preds_proba'])\n",
    "    plt.plot(recall_lstm, precision_lstm, label=f\"Time-Aware LSTM (AUC = {lstm_test_results['pr_auc']:.3f})\")\n",
    "\n",
    "# Calculate baseline precision (positive class ratio)\n",
    "positive_ratio = np.mean(lgbm_test_results['labels']) if lgbm_test_results['labels'] is not None else 0\n",
    "plt.axhline(positive_ratio, color='k', linestyle='--', label=f'Baseline ({positive_ratio:.3f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.ylim([0.0, 1.05]) # Ensure y-axis starts at 0\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(results_dir, 'time_aware_lstm_roc_pr_curves.png'))\n",
    "plt.show()\n",
    "\n",
    "# Print final comparison\n",
    "print(\"--- Model Comparison ---\")\n",
    "print(f\"LightGBM Baseline: ROC AUC = {lgbm_test_results['roc_auc']:.4f}, PR AUC = {lgbm_test_results['pr_auc']:.4f}\")\n",
    "print(f\"Time-Aware LSTM:   ROC AUC = {lstm_test_results['roc_auc']:.4f}, PR AUC = {lstm_test_results['pr_auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Analyze Attention Weights (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get attention weights (assuming it's defined in temporal_modeling.py or here)\n",
    "# def get_attention_weights(model, dataloader, device):\n",
    "#     model.eval()\n",
    "#     all_weights = []\n",
    "#     with torch.no_grad():\n",
    "#         for batch in dataloader:\n",
    "#             sequences = batch['sequences'].to(device)\n",
    "#             intervals = batch['intervals'].to(device)\n",
    "#             # Need to modify model forward pass to return weights\n",
    "#             _, weights = model(sequences, intervals) # Assuming model returns logits, weights\n",
    "#             all_weights.append(weights.cpu().numpy())\n",
    "#     return np.concatenate(all_weights, axis=0)\n",
    "\n",
    "# # Get attention weights for the test set\n",
    "# if test_loader:\n",
    "#     try:\n",
    "#         # Modify the model's forward pass temporarily or permanently to return weights\n",
    "#         # This requires adjusting the SimplifiedTimeAwareLSTM definition\n",
    "#         # For now, we'll skip this visualization as it requires model changes\n",
    "#         logger.warning(\"Attention weight analysis requires model modification to return weights. Skipping.\")\n",
    "#         # attention_weights = get_attention_weights(model, test_loader, device)\n",
    "#         # logger.info(f\"Retrieved attention weights with shape: {attention_weights.shape}\")\n",
    "#         \n",
    "#         # # Visualize attention for an example patient\n",
    "#         # example_idx = 0\n",
    "#         # example_attn = attention_weights[example_idx].flatten()\n",
    "#         # example_ts = timestamps[test_ids[example_idx]].flatten()\n",
    "#         \n",
    "#         # plt.figure(figsize=(12, 4))\n",
    "#         # plt.bar(range(len(example_attn)), example_attn)\n",
    "#         # plt.xticks(range(len(example_attn)), [f\"{t:.1f}h\" for t in example_ts], rotation=45)\n",
    "#         # plt.xlabel(\"Time Since Admission (Hours)\")\n",
    "#         # plt.ylabel(\"Attention Weight\")\n",
    "#         # plt.title(f\"Attention Weights Over Time for Example Patient {test_ids[example_idx]}\")\n",
    "#         # plt.tight_layout()\n",
    "#         # plt.show()\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"Error during attention analysis: {e}\", exc_info=True)\n",
    "# else:\n",
    "#     logger.warning(\"Cannot analyze attention weights as test_loader is not available.\")\n",
    "logger.info(\"Attention weight analysis skipped (requires model modification).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Discussion and Conclusion\n",
    "\n",
    "**Findings:**\n",
    "*   Compare the ROC AUC and PR AUC scores between the Time-Aware LSTM and the LightGBM baseline. Did explicitly modeling time intervals improve performance on this synthetic, temporally-aware dataset?\n",
    "*   Analyze the training curves. Did the LSTM converge well? Was there overfitting?\n",
    "*   (If attention analysis was performed) Did the attention mechanism focus on specific time points? Did these align with significant changes in the synthetic feature trajectories?\n",
    "\n",
    "**Limitations:**\n",
    "*   **Synthetic Data:** The primary limitation is the use of synthetically generated temporal sequences based on aggregated final values. This does *not* reflect true patient trajectories and likely oversimplifies the temporal dynamics. The goal here was purely to demonstrate the *mechanics* of the Time-Aware LSTM and compare it to a static baseline on data *designed* to have temporal structure.\n",
    "*   **MIMIC-III Demo Size:** The small size of the demo dataset limits the statistical significance of the results and the model's ability to learn complex patterns.\n",
    "*   **Hyperparameter Tuning:** Limited hyperparameter tuning was performed for both models.\n",
    "*   **Feature Engineering:** The synthetic generation process is basic. Real-world application requires sophisticated feature engineering from raw time-series data.\n",
    "\n",
    "**Implications for MLOps:**\n",
    "*   **Data Pipeline Complexity:** Handling true temporal data (extracting sequences, aligning timestamps, handling missing values within sequences) significantly increases the complexity of the data processing pipeline compared to using static/aggregated features.\n",
    "*   **Feature Store Integration:** A feature store becomes even more critical for managing time-dependent features and ensuring consistency between training and inference (point-in-time correctness).\n",
    "*   **Model Training Infrastructure:** Training deep learning models like LSTMs requires more computational resources (potentially GPUs) and different infrastructure compared to tree-based models.\n",
    "*   **Monitoring:** Monitoring temporal models involves tracking sequence input drift and potentially concept drift related to temporal patterns, adding complexity to the monitoring strategy.\n",
    "\n",
    "**Conclusion:** This notebook demonstrated the implementation of a Time-Aware LSTM, highlighting its potential to leverage temporal information in EHR data. While the results on synthetic data show [mention observed difference, e.g., 'a slight improvement' or 'comparable performance'] compared to the LightGBM baseline, applying this to real, complex EHR sequences is necessary to truly evaluate its benefits. The exercise underscores the significant increase in MLOps complexity when moving from static to temporal modeling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
